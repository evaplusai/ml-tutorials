{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model Autocomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probabilistic Language model - calculate N-grams probabilities for usig in autocomplete corpus\n",
    "###### The data is from Kaggle recipe: https://www.kaggle.com/shuyangli94/food-com-recipes-and-user-interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r'RAW_recipes.csv'\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>minutes</th>\n",
       "      <th>contributor_id</th>\n",
       "      <th>submitted</th>\n",
       "      <th>tags</th>\n",
       "      <th>nutrition</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>steps</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>n_ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>arriba   baked winter squash mexican style</td>\n",
       "      <td>137739</td>\n",
       "      <td>55</td>\n",
       "      <td>47892</td>\n",
       "      <td>2005-09-16</td>\n",
       "      <td>['60-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>[51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>['make a choice and proceed with recipe', 'dep...</td>\n",
       "      <td>autumn is my favorite time of year to cook! th...</td>\n",
       "      <td>['winter squash', 'mexican seasoning', 'mixed ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>a bit different  breakfast pizza</td>\n",
       "      <td>31490</td>\n",
       "      <td>30</td>\n",
       "      <td>26278</td>\n",
       "      <td>2002-06-17</td>\n",
       "      <td>['30-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>[173.4, 18.0, 0.0, 17.0, 22.0, 35.0, 1.0]</td>\n",
       "      <td>9</td>\n",
       "      <td>['preheat oven to 425 degrees f', 'press dough...</td>\n",
       "      <td>this recipe calls for the crust to be prebaked...</td>\n",
       "      <td>['prepared pizza crust', 'sausage patty', 'egg...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>all in the kitchen  chili</td>\n",
       "      <td>112140</td>\n",
       "      <td>130</td>\n",
       "      <td>196586</td>\n",
       "      <td>2005-02-25</td>\n",
       "      <td>['time-to-make', 'course', 'preparation', 'mai...</td>\n",
       "      <td>[269.8, 22.0, 32.0, 48.0, 39.0, 27.0, 5.0]</td>\n",
       "      <td>6</td>\n",
       "      <td>['brown ground beef in large pot', 'add choppe...</td>\n",
       "      <td>this modified version of 'mom's' chili was a h...</td>\n",
       "      <td>['ground beef', 'yellow onions', 'diced tomato...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>alouette  potatoes</td>\n",
       "      <td>59389</td>\n",
       "      <td>45</td>\n",
       "      <td>68585</td>\n",
       "      <td>2003-04-14</td>\n",
       "      <td>['60-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>[368.1, 17.0, 10.0, 2.0, 14.0, 8.0, 20.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>['place potatoes in a large pot of lightly sal...</td>\n",
       "      <td>this is a super easy, great tasting, make ahea...</td>\n",
       "      <td>['spreadable cheese with garlic and herbs', 'n...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>amish  tomato ketchup  for canning</td>\n",
       "      <td>44061</td>\n",
       "      <td>190</td>\n",
       "      <td>41706</td>\n",
       "      <td>2002-10-25</td>\n",
       "      <td>['weeknight', 'time-to-make', 'course', 'main-...</td>\n",
       "      <td>[352.9, 1.0, 337.0, 23.0, 3.0, 0.0, 28.0]</td>\n",
       "      <td>5</td>\n",
       "      <td>['mix all ingredients&amp; boil for 2 1 / 2 hours ...</td>\n",
       "      <td>my dh's amish mother raised him on this recipe...</td>\n",
       "      <td>['tomato juice', 'apple cider vinegar', 'sugar...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name      id  minutes  \\\n",
       "0  arriba   baked winter squash mexican style  137739       55   \n",
       "1            a bit different  breakfast pizza   31490       30   \n",
       "2                   all in the kitchen  chili  112140      130   \n",
       "3                          alouette  potatoes   59389       45   \n",
       "4          amish  tomato ketchup  for canning   44061      190   \n",
       "\n",
       "   contributor_id   submitted  \\\n",
       "0           47892  2005-09-16   \n",
       "1           26278  2002-06-17   \n",
       "2          196586  2005-02-25   \n",
       "3           68585  2003-04-14   \n",
       "4           41706  2002-10-25   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['60-minutes-or-less', 'time-to-make', 'course...   \n",
       "1  ['30-minutes-or-less', 'time-to-make', 'course...   \n",
       "2  ['time-to-make', 'course', 'preparation', 'mai...   \n",
       "3  ['60-minutes-or-less', 'time-to-make', 'course...   \n",
       "4  ['weeknight', 'time-to-make', 'course', 'main-...   \n",
       "\n",
       "                                    nutrition  n_steps  \\\n",
       "0       [51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]       11   \n",
       "1   [173.4, 18.0, 0.0, 17.0, 22.0, 35.0, 1.0]        9   \n",
       "2  [269.8, 22.0, 32.0, 48.0, 39.0, 27.0, 5.0]        6   \n",
       "3   [368.1, 17.0, 10.0, 2.0, 14.0, 8.0, 20.0]       11   \n",
       "4   [352.9, 1.0, 337.0, 23.0, 3.0, 0.0, 28.0]        5   \n",
       "\n",
       "                                               steps  \\\n",
       "0  ['make a choice and proceed with recipe', 'dep...   \n",
       "1  ['preheat oven to 425 degrees f', 'press dough...   \n",
       "2  ['brown ground beef in large pot', 'add choppe...   \n",
       "3  ['place potatoes in a large pot of lightly sal...   \n",
       "4  ['mix all ingredients& boil for 2 1 / 2 hours ...   \n",
       "\n",
       "                                         description  \\\n",
       "0  autumn is my favorite time of year to cook! th...   \n",
       "1  this recipe calls for the crust to be prebaked...   \n",
       "2  this modified version of 'mom's' chili was a h...   \n",
       "3  this is a super easy, great tasting, make ahea...   \n",
       "4  my dh's amish mother raised him on this recipe...   \n",
       "\n",
       "                                         ingredients  n_ingredients  \n",
       "0  ['winter squash', 'mexican seasoning', 'mixed ...              7  \n",
       "1  ['prepared pizza crust', 'sausage patty', 'egg...              6  \n",
       "2  ['ground beef', 'yellow onions', 'diced tomato...             13  \n",
       "3  ['spreadable cheese with garlic and herbs', 'n...             11  \n",
       "4  ['tomato juice', 'apple cider vinegar', 'sugar...              8  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name'][:1000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_list = df['name'][:10].rename_axis('ID').values\n",
    "#corpus_list = df['name'][:10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arriba   baked winter squash mexican style',\n",
       " 'a bit different  breakfast pizza',\n",
       " 'all in the kitchen  chili',\n",
       " 'alouette  potatoes',\n",
       " 'amish  tomato ketchup  for canning',\n",
       " 'apple a day  milk shake',\n",
       " 'aww  marinated olives',\n",
       " 'backyard style  barbecued ribs',\n",
       " 'bananas 4 ice cream  pie',\n",
       " 'beat this  banana bread']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sentences(data):\n",
    "    sentences = []\n",
    "    for key, value in data.iteritems(): \n",
    "         sentences.append(value)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arriba   baked winter squash mexican style',\n",
       " 'a bit different  breakfast pizza',\n",
       " 'all in the kitchen  chili',\n",
       " 'alouette  potatoes',\n",
       " 'amish  tomato ketchup  for canning',\n",
       " 'apple a day  milk shake',\n",
       " 'aww  marinated olives',\n",
       " 'backyard style  barbecued ribs',\n",
       " 'bananas 4 ice cream  pie',\n",
       " 'beat this  banana bread']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_list = get_sentences(df['name'][:10])\n",
    "corpus_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_data(data): \n",
    "    tokenized = []\n",
    "    for sentence in data:\n",
    "        data = str(data).lower()   \n",
    "        tokens = nltk.word_tokenize(data)\n",
    "        tokenized.append(tokens)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = get_tokenized_data(corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(87)\n",
    "random.shuffle(corpus)\n",
    "\n",
    "train_size = int(len(corpus) * 0.8)\n",
    "train_data = corpus[0:train_size]\n",
    "test_data = corpus[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(corpus):\n",
    "    word_counts = {}\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        for token in sentence:            \n",
    "            if not token in word_counts: \n",
    "                word_counts[token] = 1 \n",
    "            else:\n",
    "                word_counts[token] += 1    \n",
    "    \n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_over_threshold(tokenized, count_threshold):    \n",
    "   \n",
    "    closed_vocab = []\n",
    "   \n",
    "    word_counts = count_words(tokenized)\n",
    "    \n",
    "    for word, cnt in word_counts.items():\n",
    "        if cnt >= count_threshold:\n",
    "           \n",
    "            closed_vocab.append(word)\n",
    "  \n",
    "    return closed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all words below treshold to replace with <unk>\n",
    "def replace_below_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "    \n",
    "    vocabulary = set(vocabulary)    \n",
    "    \n",
    "    replaced_tokenized_sentences = []    \n",
    "   \n",
    "    for sentence in tokenized_sentences:\n",
    "    \n",
    "        replaced_sentence = []\n",
    " \n",
    "        for token in sentence:\n",
    "            if token in vocabulary:\n",
    "                replaced_sentence.append(token)\n",
    "            else:               \n",
    "                replaced_sentence.append(unknown_token)\n",
    "     \n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "        \n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data, count_threshold):\n",
    "    \n",
    "    vocabulary = get_words_over_threshold(train_data, count_threshold)    \n",
    "    \n",
    "    train_data_replaced = replace_below_words_by_unk(train_data, vocabulary, unknown_token=\"<unk>\")\n",
    "        \n",
    "    test_data_replaced = replace_below_words_by_unk(test_data, vocabulary, unknown_token=\"<unk>\")\n",
    "       \n",
    "    return train_data_replaced, test_data_replaced, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n",
    "                                                                        test_data, \n",
    "                                                                        minimum_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Language Model - N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
    "    \n",
    "    n_grams = {}\n",
    "   \n",
    "    for sentence in data:        \n",
    "       \n",
    "        sentence = [start_token] * n + sentence + [end_token]\n",
    "        \n",
    "        sentence = tuple(sentence)        \n",
    "        \n",
    "        for i in range(len(sentence)-n+1): \n",
    "           \n",
    "            n_gram = sentence[i:i+n]\n",
    "            \n",
    "            if n_gram in n_grams: \n",
    "               \n",
    "                n_grams[n_gram] += 1\n",
    "            else:              \n",
    "                n_grams[n_gram] = 1    \n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_probability(word, previous_n_gram, \n",
    "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):   \n",
    "   \n",
    "    previous_n_gram = tuple(previous_n_gram)    \n",
    "   \n",
    "    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n",
    "  \n",
    "    denom = previous_n_gram_count + k * vocabulary_size\n",
    "\n",
    "    n_plus1_gram = n_gram_counts.get(previous_n_gram, 0)  \n",
    "    \n",
    "    n_plus1_gram_count = n_plus1_gram_counts.get(previous_n_gram + (word, ) ,0)        \n",
    "\n",
    "    numer = n_plus1_gram_count + 1\n",
    "  \n",
    "    probability = numer / denom\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "    \n",
    "    previous_n_gram = tuple(previous_n_gram)    \n",
    "    \n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probability = word_probability(word, previous_n_gram, \n",
    "                                           n_gram_counts, n_plus1_gram_counts, \n",
    "                                           vocabulary_size, k=k)\n",
    "        probabilities[word] = probability\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_matrix(n_plus1_gram_counts, vocabulary): \n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]    \n",
    "   \n",
    "    n_grams = []\n",
    "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        n_grams.append(n_gram)\n",
    "    n_grams = list(set(n_grams))    \n",
    "    \n",
    "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}   \n",
    "    col_index = {word:j for j, word in enumerate(vocabulary)}\n",
    "    \n",
    "    nrow = len(n_grams)\n",
    "    ncol = len(vocabulary)\n",
    "    count_matrix = np.zeros((nrow, ncol))\n",
    "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        word = n_plus1_gram[-1]\n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        i = row_index[n_gram]\n",
    "        j = col_index[word]\n",
    "        count_matrix[i, j] = count\n",
    "    \n",
    "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
    "    matrix = create_count_matrix(n_plus1_gram_counts, unique_words)\n",
    "    matrix += k\n",
    "    prob_matrix = matrix.div(matrix.sum(axis=1), axis=0)\n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    \n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
    "    sentence = tuple(sentence)\n",
    "    \n",
    "    N = len(sentence)\n",
    "  \n",
    "    product_pi = 1.0\n",
    "    \n",
    "    for t in range(n, N):       \n",
    "        n_gram = sentence[t-n:t]       \n",
    "        word = sentence[t]\n",
    "      \n",
    "        probability = word_probability(word, n_gram, \n",
    "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)\n",
    "  \n",
    "        product_pi *= (1/probability)\n",
    "   \n",
    "    perplexity = product_pi ** (1/N)  \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Autocomplete sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propose_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "  \n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "   \n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    " \n",
    "    probabilities = word_probability(previous_n_gram,n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary, k=k) \n",
    "    suggestion = None   \n",
    "    max_prob = 0    \n",
    "   \n",
    "    for word, prob in probabilities.items():\n",
    "        if start_with:\n",
    "            if not word.startswith(start_with):\n",
    "                continue \n",
    "  \n",
    "        if prob > max_prob: \n",
    "            suggestion = word        \n",
    "            max_prob = prob\n",
    "            \n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proposals(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "    model_counts = len(n_gram_counts_list)\n",
    "    suggestions = []\n",
    "    for i in range(model_counts-1):\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        \n",
    "        suggestion = propose_a_word(previous_tokens, n_gram_counts,\n",
    "                                    n_plus1_gram_counts, vocabulary,\n",
    "                                    k=k, start_with=start_with)\n",
    "        suggestions.append(suggestion)\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating n-gram counts with n = 1 ...\n",
      "Creating n-gram counts with n = 2 ...\n",
      "Creating n-gram counts with n = 3 ...\n",
      "Creating n-gram counts with n = 4 ...\n",
      "Creating n-gram counts with n = 5 ...\n"
     ]
    }
   ],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    print(\"Creating n-gram counts with n =\", n, \"...\")\n",
    "    n_model_counts = count_n_grams(train_data_processed, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "word_probability() missing 1 required positional argument: 'vocabulary_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-ad1f35f21483>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprevious_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"all\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"in\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"the\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_proposals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprevious_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_gram_counts_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"The phrase words are {previous_tokens}, the suggestions are:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-45f58fb68d88>\u001b[0m in \u001b[0;36mget_proposals\u001b[1;34m(previous_tokens, n_gram_counts_list, vocabulary, k, start_with)\u001b[0m\n\u001b[0;32m      8\u001b[0m         suggestion = propose_a_word(previous_tokens, n_gram_counts,\n\u001b[0;32m      9\u001b[0m                                     \u001b[0mn_plus1_gram_counts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                                     k=k, start_with=start_with)\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0msuggestions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuggestion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msuggestions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-32cf627ac487>\u001b[0m in \u001b[0;36mpropose_a_word\u001b[1;34m(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k, start_with)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     probabilities = word_probability(previous_n_gram,n_gram_counts, n_plus1_gram_counts,\n\u001b[1;32m----> 8\u001b[1;33m                                            vocabulary, k=k) \n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0msuggestion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmax_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: word_probability() missing 1 required positional argument: 'vocabulary_size'"
     ]
    }
   ],
   "source": [
    "previous_tokens = [\"all\", \"in\", \"the\"]\n",
    "test_4 = get_proposals(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The phrase words are {previous_tokens}, the suggestions are:\")\n",
    "display(test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
